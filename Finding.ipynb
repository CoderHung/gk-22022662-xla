{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     71\u001b[39m wait_time = \u001b[32m2000\u001b[39m    \u001b[38;5;66;03m# 2 seconds between methods (press any key to continue faster)\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, method \u001b[38;5;129;01min\u001b[39;00m methods.items():\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Apply multi-scale template matching\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     location, size, confidence = \u001b[43mmulti_scale_template_matching\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlarge_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscale_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     81\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: No match found\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mmulti_scale_template_matching\u001b[39m\u001b[34m(image, template, mask, method, scale_range, steps)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Match template\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     result = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatchTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     21\u001b[39m     result = cv2.matchTemplate(resized, template, method)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def multi_scale_template_matching(image, template, mask=None, method=cv2.TM_CCOEFF_NORMED, \n",
    "                                scale_range=(0.5, 1.5), steps=10):\n",
    "    best_scale, best_score, best_loc = None, -1, None\n",
    "    template_h, template_w = template.shape[:2]\n",
    "    \n",
    "    for scale in np.linspace(scale_range[0], scale_range[1], steps):\n",
    "        # Resize the image\n",
    "        resized = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Skip if template is larger than resized image\n",
    "        if resized.shape[0] < template_h or resized.shape[1] < template_w:\n",
    "            continue\n",
    "            \n",
    "        # Match template\n",
    "        if mask is not None:\n",
    "            result = cv2.matchTemplate(resized, template, method, mask=mask)\n",
    "        else:\n",
    "            result = cv2.matchTemplate(resized, template, method)\n",
    "        \n",
    "        # For SQDIFF methods, we look for minimum, others look for maximum\n",
    "        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "            _, max_val, _, min_loc = cv2.minMaxLoc(result)\n",
    "            current_val = max_val\n",
    "            current_loc = min_loc\n",
    "        else:\n",
    "            _, max_val, _, max_loc = cv2.minMaxLoc(result)\n",
    "            current_val = max_val\n",
    "            current_loc = max_loc\n",
    "        \n",
    "        # Update best match\n",
    "        if current_val > best_score:\n",
    "            best_scale = scale\n",
    "            best_score = current_val\n",
    "            best_loc = current_loc\n",
    "            best_size = (template_w, template_h)\n",
    "    \n",
    "    if best_scale:\n",
    "        # Scale the bounding box back to original image coordinates\n",
    "        orig_loc = (int(best_loc[0] / best_scale), int(best_loc[1] / best_scale))\n",
    "        orig_size = (int(best_size[0] / best_scale), int(best_size[1] / best_scale))\n",
    "        return orig_loc, orig_size, best_score\n",
    "    return None, None, 0\n",
    "\n",
    "# Load images\n",
    "template = cv2.imread('Finding/3-tiger.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "large_img = cv2.imread('Finding/3-clean.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Check if images loaded successfully\n",
    "if template is None or large_img is None:\n",
    "    print(\"Error: Could not load images.\")\n",
    "    exit()\n",
    "\n",
    "# Create mask for white background\n",
    "_, mask = cv2.threshold(template, 250, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# All available template matching methods\n",
    "methods = {\n",
    "    'TM_CCOEFF': cv2.TM_CCOEFF,\n",
    "    'TM_CCOEFF_NORMED': cv2.TM_CCOEFF_NORMED,\n",
    "    'TM_CCORR': cv2.TM_CCORR,\n",
    "    'TM_CCORR_NORMED': cv2.TM_CCORR_NORMED,\n",
    "    'TM_SQDIFF': cv2.TM_SQDIFF,\n",
    "    'TM_SQDIFF_NORMED': cv2.TM_SQDIFF_NORMED\n",
    "}\n",
    "\n",
    "# Display settings\n",
    "scale_percent = 30 # Smaller display size (40% of original)\n",
    "wait_time = 2000    # 2 seconds between methods (press any key to continue faster)\n",
    "\n",
    "for name, method in methods.items():\n",
    "    # Apply multi-scale template matching\n",
    "    location, size, confidence = multi_scale_template_matching(\n",
    "        large_img, template, mask, method,\n",
    "        scale_range=(0.7, 1.4), steps=20\n",
    "    )\n",
    "    \n",
    "    if location is None:\n",
    "        print(f\"{name}: No match found\")\n",
    "        continue\n",
    "    \n",
    "    # Draw rectangle on copy of original image\n",
    "    display_img = cv2.cvtColor(large_img, cv2.COLOR_GRAY2BGR)\n",
    "    top_left = (int(location[0]), int(location[1]))\n",
    "    bottom_right = (int(location[0] + size[0]), int(location[1] + size[1]))\n",
    "    cv2.rectangle(display_img, top_left, bottom_right, (0, 255, 0), 2)\n",
    "    \n",
    "    # Resize for smaller display\n",
    "    width = int(display_img.shape[1] * scale_percent / 100)\n",
    "    height = int(display_img.shape[0] * scale_percent / 100)\n",
    "    resized_img = cv2.resize(display_img, (width, height), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Add method info\n",
    "    text1 = f\"Method: {name}\"\n",
    "    text2 = f\"Confidence: {confidence:.4f}\"\n",
    "    text3 = f\"Scale: {size[0]/template.shape[1]:.2f}x\"\n",
    "    cv2.putText(resized_img, text1, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    cv2.putText(resized_img, text2, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    cv2.putText(resized_img, text3, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow('Multi-Scale Template Matching', resized_img)\n",
    "    \n",
    "    # Wait for key press or timeout\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dominant (Background) Color (RGB): (np.uint8(254), np.uint8(254), np.uint8(254))\n",
      "Background Color (RGB): (np.uint8(255), np.uint8(255), np.uint8(255))\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Load the image (object with background)\n",
    "img = cv2.imread(\"Finding/1-cone.jpg\")  # BGR format\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB for better interpretation\n",
    "\n",
    "# Flatten the image to a list of pixels\n",
    "pixels = img_rgb.reshape(-1, 3)\n",
    "\n",
    "# Find the most common color (background)\n",
    "bg_color = Counter(map(tuple, pixels)).most_common(1)[0][0]\n",
    "print(f\"Dominant (Background) Color (RGB): {bg_color}\")\n",
    "# Get corner pixels (top-left, top-right, bottom-left, bottom-right)\n",
    "corners = [\n",
    "    img_rgb[0, 0], img_rgb[0, -1],  # Top-left, Top-right\n",
    "    img_rgb[-1, 0], img_rgb[-1, -1]   # Bottom-left, Bottom-right\n",
    "]\n",
    "\n",
    "# Find the most common corner color (background)\n",
    "bg_color = Counter(map(tuple, corners)).most_common(1)[0][0]\n",
    "print(f\"Background Color (RGB): {bg_color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load template (grayscale)\n",
    "template = cv2.imread('Finding/1-cone.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Create mask (white background = ignored)\n",
    "_, mask = cv2.threshold(template, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# Stack images horizontally for comparison\n",
    "result_visual = np.hstack((template, mask))\n",
    "\n",
    "# Display\n",
    "cv2.imshow('Template (Left) vs. Mask (Right)', result_visual)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
